# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kDwvPKFxNjtPbH0nVVG_avfx9kyQzaTU

Generate basic dummy script for first cluster run
"""

import numpy as np
import pandas as pd
import scipy as sc
import torch

from math import floor
import time
import os
import matplotlib.pyplot as plt
import pickle

"""in the previous approaches, I modeled the risk state as a discrete covariate. Here, the risk states are implemented as single states which increases the number of parameters greatly but faciliates the identification of the parameters"""

RNG = np.random.default_rng(2024)
params = [1,2]
D = len(params)

#update single state #mobility states only change gradually but can jump further if the observation window is long enough
def sgl_update_eq5d_mob(params):

  return RNG.normal(loc = 1, scale = params, size = 2)

sgl_update_eq5d_mob(params)

"""# networks"""

#pip install sbi

#packages
from sbi.inference import SNLE, SNPE, prepare_for_sbi, simulate_for_sbi
from sbi.utils.get_nn_models import posterior_nn
from sbi import utils as utils
from sbi import analysis as analysis
from sbi.analysis import pairplot

from sbi.utils.user_input_checks import (
    check_sbi_inputs,
    process_prior,
    process_simulator,
)

# prior = utils.MultipleIndependent(
#     [
#         utils.BoxUniform(low=0 * torch.ones(1), high=2 * torch.ones(1)),
#         utils.BoxUniform(low=0 * torch.ones(1), high=2 * torch.ones(1)),
#         utils.BoxUniform(low=0 * torch.ones(1), high=2 * torch.ones(1)),
#         utils.BoxUniform(low=0 * torch.ones(1), high=2 * torch.ones(1))
#     ],
#      validate_args=False,
# )
prior = utils.BoxUniform(low=0 * torch.ones(D), high=4 * torch.ones(D))

### this could certainly be better written although I donÂ´t see how I can facilitate the append solution and it works...

def sim_SUR(params):
  return sgl_update_eq5d_mob(params)

sim_SUR(params).shape

#check whether prior and sim are adapted to sbi
prior, num_parameters, prior_returns_numpy = process_prior(prior)
simulator = process_simulator(
    sim_SUR,
    prior,
    prior_returns_numpy,
)
check_sbi_inputs(simulator, prior)

#load real data
with open('eq5d_wirklich_ohne7.pkl', 'rb') as f:
    raw_data = pickle.load(f)

def extract_sgl_trans(raw_data, index):

  current_id = raw_data["patient_id"][index]

  #check if this is a valid transition
  if (raw_data["patient_id"][index] != raw_data["patient_id"][index + 1]):
      print(rf"miss-specified indices at index {index}")
  else:
    #prepare transition times, ids
    trans_time = (raw_data["cycle_date"][index+1]-raw_data["cycle_date"][index]).days
    start_state = raw_data["eq5d_mobility"][index]
    end_state = raw_data["eq5d_mobility"][index + 1]

    return torch.tensor([start_state, end_state, trans_time, current_id])

extract_sgl_trans(raw_data, 12)

#get whole data set
data = {}
L = len(raw_data["patient_id"])
n_sgl_trans = 0

for l in range(L-1):
  #check that data belongs to same patient, does not contain nan
  index = l
  if (raw_data["patient_id"][l] == raw_data["patient_id"][l + 1] and np.isnan(raw_data["eq5d_mobility"][l]) == False and np.isnan(raw_data["eq5d_mobility"][l+1]) == False):
    new_set = extract_sgl_trans(raw_data, l)
    data[rf"{n_sgl_trans}"] = new_set
    n_sgl_trans = n_sgl_trans + 1

data["11"][0:2]

"""## SNLE approach"""

# Train SNLE.
inferer = SNLE(prior, show_progress_bars=True, density_estimator="mdn")
theta, x = simulate_for_sbi(simulator, prior, 100, simulation_batch_size=32)#orginally simulation_batch_size = 1000
inferer.append_simulations(theta, x).train(training_batch_size=1000);

with open("NLE_infe_test.pkl", "wb") as handle:
    pickle.dump(inferer, handle)

#load network
# inferer = SNLE(prior, show_progress_bars=True, density_estimator="mdn")
# theta, x = simulate_for_sbi(simulator, prior, 10000, simulation_batch_size=32)#orginally simulation_batch_size = 1000
with open('NLE_infe_test.pkl', 'rb') as f:
    inferer = pickle.load(f)

#num_trials = [1, 5, 15, 20]
#num_trials = [191]
num_trials = [1]
max_num_trials = max(num_trials)

# Generate multiple x_os with increasing number of trials.
xos = [torch.zeros(nt, 2) for nt in num_trials]

#insert observations into correct format
for i in range(len(num_trials)):
  nt = num_trials[i]
  for j in range(nt):
    xos[i][j] = torch.tensor(data[rf"{i}"][0:2])

# Obtain posterior samples for different number of iid xos.
nle_samples = []
num_samples = 1000

mcmc_parameters = dict(
    num_chains=50,#originally 50
    thin=5,
    warmup_steps=30,#originally 30
    init_strategy="proposal",
)
mcmc_method = "slice_np_vectorized"

posterior = inferer.build_posterior(
    mcmc_method=mcmc_method,
    mcmc_parameters=mcmc_parameters,
)

# Generate samples with MCMC given the same set of x_os as above.
for xo in xos:
    nle_samples.append(posterior.sample(sample_shape=(num_samples,), x=xo))

with open(rf"NLE_samples_{max_num_trials}obs.pkl", "wb") as handle:
    pickle.dump(nle_samples, handle)

with open(rf'NLE_samples_{max_num_trials}obs.pkl', 'rb') as f:
    nle_samples = pickle.load(f)

# Plot them in one pairplot as contours (obtained via KDE on the samples).
fig, ax = pairplot(
    nle_samples,
    #points=torch.tensor(params),
    diag="kde",
    #upper="contour",
    kde_offdiag=dict(bins=50),
    kde_diag=dict(bins=100),
    contour_offdiag=dict(levels=[0.95]),
    points_colors=["k"],
    points_offdiag=dict(marker="*", markersize=10),

)
plt.sca(ax[1, 1])
# plt.legend(
#     [f"{nt} trials" if nt > 1 else f"{nt} trial" for nt in num_trials]
#     + [r"$\theta_o$"],
#     frameon=False,
#     fontsize=12,
# );
plt.savefig(rf"NLE_posteriors_{max_num_trials}.png")

posterior_samples = nle_samples
# plot posterior samples
_ = analysis.pairplot(
    posterior_samples[0], figsize=(8, 8)
)
plt.savefig(rf"NLE_posterior_{max_num_trials}obs.png")

"""#### analysis

Simulation Based Calibration
"""

# i mendled quite a bit with the parameters in this cell, might not be suited well for changing conditions - if so, refer back to sbi tutorials

num_sbc_runs = 100  # choose a number of sbc runs, should be ~100s or ideally 1000
# generate ground truth parameters and corresponding simulated observations for SBC.
thetas = prior.sample((num_sbc_runs,))
xs = simulator(thetas)

# run SBC: for each inference we draw 1000 posterior samples. #reduced to 100 instead of 1000 for time constraints
num_posterior_samples = 10
ranks, dap_samples = analysis.run_sbc(
    thetas, xs, posterior, num_posterior_samples=num_posterior_samples
)

check_stats = analysis.check_sbc(
    ranks, thetas, dap_samples, num_posterior_samples=num_posterior_samples
)

print(
    f"kolmogorov-smirnov p-values \ncheck_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}"
)

print(
    f"c2st accuracies \ncheck_stats['c2st_ranks'] = {check_stats['c2st_ranks'].numpy()}"
)

print(f"- c2st accuracies check_stats['c2st_dap'] = {check_stats['c2st_dap'].numpy()}")

f, ax = analysis.sbc_rank_plot(
    ranks=ranks,
    num_posterior_samples=num_posterior_samples,
    plot_type="hist",
    num_bins=10,  # by passing None we use a heuristic for the number of bins.
)
plt.savefig("NLE_sbc.png")

f, ax = analysis.sbc_rank_plot(ranks, 1_000, plot_type="cdf", num_bins = 10)
plt.savefig("NLE_ECDF.png")

#compute plot recovery, fixed on current prior - was wrongly implemented up to now (i.e. 27.04 as I sampled the prior and conditional observations each time anew)
M = 10
z = torch.zeros((D,M))
contr = torch.zeros((D,M))

prior_params = prior.sample()
x_o = sim_SUR(prior_params)

for k in range(M):

  #obtain new posterior samples
  poster_samples = posterior.sample((100,), x = x_o)#this means I recompute the posterior just based on 1 observations, this should be around 1000 for comparison to the other values
  #posterior_samples_ar = np.asarray(poster_samples)

  #compute empirical means/variances/std
  mean = torch.zeros(D)
  var = torch.zeros(D)
  std = torch.zeros(D)
  var_prior = (0-50)**2 /12

  for i in range(D):
    mean[i] = torch.mean(poster_samples[:,i])
    var[i] = torch.var(poster_samples[:,i])
    std[i] = torch.std(poster_samples[:,i])

    #get scores
    z[i,k] = (mean[i]-prior_params[i])/std[i]
    contr[i,k] = 1-var[i]/var_prior

for i in range(D):
  plt.scatter(contr[i,:],z[i,:])
  plt.title("Plot recovery " + rf"$\theta_{i+1}$")
  plt.xlabel("posterior contraction")
  plt.ylabel("posterior z-score")
  plt.xlim([0,1])
  plt.ylim([-3,3])
  plt.savefig(rf"NLE_plot_recovery_theta{1+i}.png")
  plt.show()