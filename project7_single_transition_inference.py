# -*- coding: utf-8 -*-
"""project7_single_transition_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10GD4ymItxo89rDVhRMJf5e9JcocgBDli

This notebook is a prototype for my largest inference so far. The idea is to split the patient data into many more single transition observations, thus losing the relevance of patient specific data (which could be recovered somehow by including this as a covariat although this is a much weaker notion and shouldn´t work actually)
"""

import numpy as np
import pandas as pd
import scipy as sc
import torch

from math import floor
import time
import os
import matplotlib.pyplot as plt
import pickle

"""# load data"""

#load real data
with open('eq5d_wirklich_ohne7.pkl', 'rb') as f:
    raw_data = pickle.load(f)

#save as excel
# import pandas
# data.to_excel("eq5D.xlsx")

"""Extract one single patient history, i.e. relevant times and states of interest, properly labeled and in the correct shape"""

def extract_sgl_trans(raw_data, index):

  current_id = raw_data["patient_id"][index]

  #check if this is a valid transition
  if (raw_data["patient_id"][index] != raw_data["patient_id"][index + 1]):
      print(rf"miss-specified indices at index {index}")
  else:
    #prepare transition times, ids
    trans_time = (raw_data["cycle_date"][index+1]-raw_data["cycle_date"][index]).days
    start_state = raw_data["eq5d_mobility"][index]
    end_state = raw_data["eq5d_mobility"][index + 1]

  # #unify data types
  # if (type(hist_states) == np.float64):
  #   hist_states = [hist_states]


  # #assign data to list
  # indv_data["event_times"] = obs_times
  # #indv_data["hist_states"] = hist_states[1:] #obtain number of transitions as len()
  # indv_data["hist_states"] = hist_states #obtain number of transitions as len()
  # indv_data["hist_times"] = hist_times



  # data[rf"{current_id}"] = indv_data

    return torch.tensor([start_state, end_state, trans_time, current_id])

extract_sgl_trans(raw_data, 12)

#get whole data set
data = {}
L = len(raw_data["patient_id"])
n_sgl_trans = 0

for l in range(L-1):
  #check that data belongs to same patient, does not contain nan
  index = l
  if (raw_data["patient_id"][l] == raw_data["patient_id"][l + 1] and np.isnan(raw_data["eq5d_mobility"][l]) == False and np.isnan(raw_data["eq5d_mobility"][l+1]) == False):
    new_set = extract_sgl_trans(raw_data, l)
    data[rf"{n_sgl_trans}"] = new_set
    n_sgl_trans = n_sgl_trans + 1

print(n_sgl_trans)
#there are 1153 valid single transitions, including those in between the same state

print(data["1"][0:3])

"""Some basic descriptive statistics"""

#extract mean transition time
sgl_data = np.zeros((n_sgl_trans,4))
for j in range(n_sgl_trans):
  sgl_data[j,:]  = data[rf"{j}"]

print(rf"The mean transition time is {np.mean(sgl_data[:,2])}")
print(rf"The min transition time is {np.min(sgl_data[:,2])}")
print(rf"The max transition time is {np.max(sgl_data[:,2])}")

#how likely are transitions along neighbours
jump_diff = np.zeros(5)
for j in range(n_sgl_trans):
  dummy = int(abs(data[rf"{j}"][1]-data[rf"{j}"][0]))
  jump_diff[dummy] = jump_diff[dummy] +1

#plot
plt.step(np.linspace(0,4,5),jump_diff, where = "mid")
plt.show()

#how are the observations distributed?
plt.hist(sgl_data[:,2], bins = 100)
plt.xlim([0,500])
plt.show()
print(sgl_data[:,2])

"""# simulate data"""

#initialise parameters
possible_states = [1,2,3,4,5]
N = len(possible_states)
#just neighboring states - if there are larger jumps this simply means that the jumps in between were unobserved
params = np.repeat(0.5,8) #1 to 2, #2 to 1,3
D = len(params)
t_0 = 0 #just a basic initial time
t_max = 2016 #maximum transition time encountered in the data
RNG = np.random.default_rng(2024)

"""in the previous approaches, I modeled the risk state as a discrete covariate. Here, the risk states are implemented as single states which increases the number of parameters greatly but faciliates the identification of the parameters"""

#update single state #mobility states only change gradually but can jump further if the observation window is long enough
def sgl_update_eq5d_mob(params, state, t_0, t_max):

  #compute potential event times
  #just neighboring states
  if (int(state) == 1):
    firing_times = np.repeat(1000.0,1) #just a high threshold
    lmbda = params[0]
    firing_times[0] = RNG.exponential(scale = (1/lmbda))
  elif (int(state) == 5):
    firing_times = np.repeat(1000.0,1) #just a high threshold
    lmbda = params[7]
    firing_times[0] = RNG.exponential(scale = (1/lmbda))
  else:
    firing_times = np.repeat(1000.0,2) #just a high threshold
    for i in range(2):
      lmbda = params[2*int(state)-3+i]
      firing_times[i] = RNG.exponential(scale = (1/lmbda))

  #obtain transition
  # T_firing = np.min(firing_times)
  T_firing = 7 + floor(np.min(firing_times)*(55-7)) #try to approach the actual range of waiting times but keeping in mind that there might be unobserved transitions
  new_t = t_0 + T_firing
  if (np.argmin(firing_times) < state -1):
    new_state = np.argmin(firing_times) + 1
  else:
    new_state = np.argmin(firing_times) + 2

  #eligible simulation?
  if (new_t < t_max):
    return new_state, new_t
  else:
    return state, t_max

#simulate whole patient history
def sim_sgl_trans_mob(params, state_0, t_0, t_max):

  #original approach to model several transitions but I can also just draw a randomly whether there is another transition, this can be adpated more easily to the actual distribution of transitions
  # obs_time = 7 + floor(RNG.exponential(scale = (1/0.6))) # very arbitrary selected to make it more likely for multiple transitions between 2 observations

  new_state, new_t = sgl_update_eq5d_mob(params, state_0, t_0, t_max)
  #1 unobserved transition
  if (RNG.uniform() < 0.25):
    new_state, new_t = sgl_update_eq5d_mob(params, new_state, new_t, t_max)

  return torch.tensor([state_0, new_state, new_t])

sim_sgl_trans_mob(params, 1, t_0, t_max)

"""# networks"""

#pip install sbi

#packages
from sbi.inference import SNLE, SNPE, prepare_for_sbi, simulate_for_sbi
from sbi.utils.get_nn_models import posterior_nn
from sbi import utils as utils
from sbi import analysis as analysis
from sbi.analysis import pairplot

from sbi.utils.user_input_checks import (
    check_sbi_inputs,
    process_prior,
    process_simulator,
)

prior = utils.MultipleIndependent(
    [
        utils.BoxUniform(low=0 * torch.ones(1), high=15 * torch.ones(1)),
        utils.BoxUniform(low=0 * torch.ones(1), high=7.5 * torch.ones(1)),
        utils.BoxUniform(low=0 * torch.ones(1), high=2 * torch.ones(1)),
        utils.BoxUniform(low=0 * torch.ones(1), high=15 * torch.ones(1)),
        utils.BoxUniform(low=5 * torch.ones(1), high=20 * torch.ones(1)),
        utils.BoxUniform(low=0 * torch.ones(1), high=20 * torch.ones(1)),
        utils.BoxUniform(low=10 * torch.ones(1), high=40* torch.ones(1)),
        utils.BoxUniform(low=0 * torch.ones(1), high=15 * torch.ones(1))
    ],
     validate_args=False,
)
#prior = utils.BoxUniform(low=0 * torch.ones(D), high=20 * torch.ones(D))

### this could certainly be better written although I don´t see how I can facilitate the append solution and it works...

def sim_SUR(params):
  #draw intial states
  state_0 = 1 + RNG.binomial(4, 0.3)
  out = sim_sgl_trans_mob(params, state_0, t_0, t_max)
  return out

sim_SUR(params).shape

#check whether prior and sim are adapted to sbi
prior, num_parameters, prior_returns_numpy = process_prior(prior)
simulator = process_simulator(
    sim_SUR,
    prior,
    prior_returns_numpy,
)
check_sbi_inputs(simulator, prior)

"""## SNLE approach"""

# Train SNLE.
inferer = SNLE(prior, show_progress_bars=True, density_estimator="mdn")
theta, x = simulate_for_sbi(simulator, prior, 100000, simulation_batch_size=32)#orginally simulation_batch_size = 1000
inferer.append_simulations(theta, x).train(training_batch_size=1000);

with open("NLE_infe.pkl", "wb") as handle:
    pickle.dump(inferer, handle)

#load network
# inferer = SNLE(prior, show_progress_bars=True, density_estimator="mdn")
# theta, x = simulate_for_sbi(simulator, prior, 10000, simulation_batch_size=32)#orginally simulation_batch_size = 1000
with open('NLE_infe.pkl', 'rb') as f:
    inferer = pickle.load(f)

#num_trials = [1, 5, 15, 20]
#num_trials = [191]
num_trials = [50]
max_num_trials = max(num_trials)

# Generate multiple x_os with increasing number of trials.
xos = [torch.zeros(nt, 3) for nt in num_trials]

#insert observations into correct format
for i in range(len(num_trials)):
  nt = num_trials[i]
  for j in range(nt):
    xos[i][j] = data[rf"{j}"][0:3]

# Obtain posterior samples for different number of iid xos.
nle_samples = []
num_samples = 1000

mcmc_parameters = dict(
    num_chains=50,#originally 50
    thin=5,
    warmup_steps=30,#originally 30
    init_strategy="proposal",
)
mcmc_method = "slice_np_vectorized"

posterior = inferer.build_posterior(
    mcmc_method=mcmc_method,
    mcmc_parameters=mcmc_parameters,
)

# Generate samples with MCMC given the same set of x_os as above.
for xo in xos:
    nle_samples.append(posterior.sample(sample_shape=(num_samples,), x=xo))

with open(rf"NLE_samples_{max_num_trials}obs.pkl", "wb") as handle:
    pickle.dump(nle_samples, handle)

with open(rf'NLE_samples_{max_num_trials}obs.pkl', 'rb') as f:
    nle_samples = pickle.load(f)

print(nle_samples)
print(nle_samples[0].shape)

# Plot them in one pairplot as contours (obtained via KDE on the samples).
fig, ax = pairplot(
    nle_samples,
    #points=torch.tensor(params),
    diag="kde",
    #upper="contour",
    kde_offdiag=dict(bins=50),
    kde_diag=dict(bins=100),
    contour_offdiag=dict(levels=[0.95]),
    points_colors=["k"],
    points_offdiag=dict(marker="*", markersize=10),

)
plt.sca(ax[1, 1])
plt.legend(
    [f"{nt} trials" if nt > 1 else f"{nt} trial" for nt in num_trials]
    + [r"$\theta_o$"],
    frameon=False,
    fontsize=12,
);
plt.savefig(rf"NLE_posteriors_{max_num_trials}.png")

posterior_samples = nle_samples
# plot posterior samples
_ = analysis.pairplot(
    posterior_samples[0], figsize=(8, 8)
)
plt.savefig(rf"NLE_posterior_{max_num_trials}obs.png")

"""#### analysis

Simulation Based Calibration
"""

# i mendled quite a bit with the parameters in this cell, might not be suited well for changing conditions - if so, refer back to sbi tutorials

num_sbc_runs = 100  # choose a number of sbc runs, should be ~100s or ideally 1000
# generate ground truth parameters and corresponding simulated observations for SBC.
thetas = prior.sample((num_sbc_runs,))
xs = simulator(thetas)

# run SBC: for each inference we draw 1000 posterior samples. #reduced to 100 instead of 1000 for time constraints
num_posterior_samples = 300
ranks, dap_samples = analysis.run_sbc(
    thetas, xs, posterior, num_posterior_samples=num_posterior_samples
)

check_stats = analysis.check_sbc(
    ranks, thetas, dap_samples, num_posterior_samples=num_posterior_samples
)

print(
    f"kolmogorov-smirnov p-values \ncheck_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}"
)

print(
    f"c2st accuracies \ncheck_stats['c2st_ranks'] = {check_stats['c2st_ranks'].numpy()}"
)

print(f"- c2st accuracies check_stats['c2st_dap'] = {check_stats['c2st_dap'].numpy()}")

f, ax = analysis.sbc_rank_plot(
    ranks=ranks,
    num_posterior_samples=num_posterior_samples,
    plot_type="hist",
    num_bins=10,  # by passing None we use a heuristic for the number of bins.
)
plt.savefig("NLE_sbc.png")

f, ax = analysis.sbc_rank_plot(ranks, 1_000, plot_type="cdf", num_bins = 10)
plt.savefig("NLE_ECDF.png")

#compute plot recovery, fixed on current prior - was wrongly implemented up to now (i.e. 27.04 as I sampled the prior and conditional observations each time anew)
M = 100
z = torch.zeros((D,M))
contr = torch.zeros((D,M))

prior_params = prior.sample()
x_o = sim_SUR(prior_params)

for k in range(M):

  #obtain new posterior samples
  poster_samples = posterior.sample((100,), x = x_o)#this means I recompute the posterior just based on 1 observations, this should be around 1000 for comparison to the other values
  #posterior_samples_ar = np.asarray(poster_samples)

  #compute empirical means/variances/std
  mean = torch.zeros(D)
  var = torch.zeros(D)
  std = torch.zeros(D)
  var_prior = (0-50)**2 /12

  for i in range(D):
    mean[i] = torch.mean(poster_samples[:,i])
    var[i] = torch.var(poster_samples[:,i])
    std[i] = torch.std(poster_samples[:,i])

    #get scores
    z[i,k] = (mean[i]-prior_params[i])/std[i]
    contr[i,k] = 1-var[i]/var_prior

for i in range(D):
  plt.scatter(contr[i,:],z[i,:])
  plt.title("Plot recovery " + rf"$\theta_{i+1}$")
  plt.xlabel("posterior contraction")
  plt.ylabel("posterior z-score")
  plt.xlim([0,1])
  plt.ylim([-3,3])
  plt.savefig(rf"NLE_plot_recovery_theta{1+i}.png")
  plt.show()

"""## SNPE approach

#### load necessary functions
"""

#couldn´t load this from the package, so I copied it directly in
from torch import nn
from typing import Any, Callable, Optional

from sbi.neural_nets.classifier import (
    build_linear_classifier,
    build_mlp_classifier,
    build_resnet_classifier,
)
from sbi.neural_nets.flow import (
    build_made,
    build_maf,
    build_maf_rqs,
    build_nsf,
    #build_zuko_maf,
)
from sbi.neural_nets.mdn import build_mdn
from sbi.neural_nets.mnle import build_mnle

#somehow posterior_nn is not accessible for me, so I copied the code directly from the source
def posterior_nn(
    model: str,
    z_score_theta: Optional[str] = "independent",
    z_score_x: Optional[str] = "independent",
    hidden_features: int = 50,
    num_transforms: int = 5,
    num_bins: int = 10,
    embedding_net: nn.Module = nn.Identity(),
    num_components: int = 10,
    **kwargs: Any,
) -> Callable:
    r"""
    Returns a function that builds a density estimator for learning the posterior.

    This function will usually be used for SNPE. The returned function is to be passed
    to the inference class when using the flexible interface.

    Args:
        model: The type of density estimator that will be created. One of [`mdn`,
            `made`, `maf`, `maf_rqs`, `nsf`].
        z_score_theta: Whether to z-score parameters $\theta$ before passing them into
            the network, can take one of the following:
            - `none`, or None: do not z-score.
            - `independent`: z-score each dimension independently.
            - `structured`: treat dimensions as related, therefore compute mean and std
            over the entire batch, instead of per-dimension. Should be used when each
            sample is, for example, a time series or an image.
        z_score_x: Whether to z-score simulation outputs $x$ before passing them into
            the network, same options as z_score_theta.
        hidden_features: Number of hidden features.
        num_transforms: Number of transforms when a flow is used. Only relevant if
            density estimator is a normalizing flow (i.e. currently either a `maf` or a
            `nsf`). Ignored if density estimator is a `mdn` or `made`.
        num_bins: Number of bins used for the splines in `nsf`. Ignored if density
            estimator not `nsf`.
        embedding_net: Optional embedding network for simulation outputs $x$. This
            embedding net allows to learn features from potentially high-dimensional
            simulation outputs.
        num_components: Number of mixture components for a mixture of Gaussians.
            Ignored if density estimator is not an mdn.
        kwargs: additional custom arguments passed to downstream build functions.
    """

    kwargs = dict(
        zip(
            (
                "z_score_x",
                "z_score_y",
                "hidden_features",
                "num_transforms",
                "num_bins",
                "embedding_net",
                "num_components",
            ),
            (
                z_score_theta,
                z_score_x,
                hidden_features,
                num_transforms,
                num_bins,
                embedding_net,
                num_components,
            ),
        ),
        **kwargs,
    )

    def build_fn_snpe_a(batch_theta, batch_x, num_components):
        """Build function for SNPE-A

        Extract the number of components from the kwargs, such that they are exposed as
        a kwargs, offering the possibility to later override this kwarg with
        `functools.partial`. This is necessary in order to make sure that the MDN in
        SNPE-A only has one component when running the Algorithm 1 part.
        """
        return build_mdn(
            batch_x=batch_theta,
            batch_y=batch_x,
            num_components=num_components,
            **kwargs,
        )

    def build_fn(batch_theta, batch_x):
        if model == "mdn":
            return build_mdn(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        elif model == "made":
            return build_made(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        elif model == "maf":
            return build_maf(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        elif model == "maf_rqs":
            return build_maf_rqs(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        elif model == "nsf":
            return build_nsf(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        elif model == "zuko_maf":
            return build_zuko_maf(batch_x=batch_theta, batch_y=batch_x, **kwargs)
        else:
            raise NotImplementedError

    if model == "mdn_snpe_a":
        if num_components != 10:
            raise ValueError(
                "You set `num_components`. For SNPE-A, this has to be done at "
                "instantiation of the inference object, i.e. "
                "`inference = SNPE_A(..., num_components=20)`"
            )
        kwargs.pop("num_components")

    return build_fn_snpe_a if model == "mdn_snpe_a" else build_fn

"""#### actual setup"""

#trying to fix the training phase
x_dim = 3

# we need to fix the maximum number of trials.
#num_trials = [1, 5, 15, 20]
num_trials = [50]
max_num_trials = max(num_trials)

# construct training data set: we want to cover the full range of possible number of
# trials
num_training_samples = 1000
theta = prior.sample((num_training_samples,))


### this was the origianl approach form the tutorial, this does NOT include any actual data - OR are these training samples the simulations?
#there are certainly smarter ways to construct the training data set, but we go with a
#for loop here for illustration purposes.
x = torch.ones(num_training_samples * max_num_trials, max_num_trials, x_dim) * float(
    "nan"
)
for i in range(num_training_samples):
    xi = simulator(theta[i].repeat(max_num_trials, 1))
    for j in range(max_num_trials):
        x[i * max_num_trials + j, : j + 1, :] = xi[: j + 1, :]

###attempt to include actual data
# x = torch.ones(max_num_trials, x_dim) * float("nan")

# for j in range(max_num_trials):
#   a = np.append(data_ext[rf"{list_of_ids[j]}"]["hist_states"],data_ext[rf"{list_of_ids[j]}"]["hist_times"])
#   x[j, :] = torch.tensor(a)

theta = theta.repeat_interleave(max_num_trials, dim=0)

from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding

# embedding
latent_dim = 10
single_trial_net = FCEmbedding(
    input_dim=x_dim,
    num_hiddens=40,
    num_layers=2,
    output_dim=latent_dim,
)
embedding_net = PermutationInvariantEmbedding(
    single_trial_net,
    trial_net_output_dim=latent_dim,
    # NOTE: post-embedding is not needed really.
    num_layers=1,
    num_hiddens=10,
    output_dim=10,
)

# we choose a simple MDN as the density estimator.
# NOTE: we turn off z-scoring of the data, as we used NaNs for the missing trials.
density_estimator = posterior_nn("mdn", embedding_net=embedding_net, z_score_x="none")

inference = SNPE(prior, density_estimator=density_estimator)
# NOTE: we don't exclude invalid x because we used NaNs for the missing trials.
inference.append_simulations(
    theta,
    x,
    exclude_invalid_x=False,
).train(training_batch_size=1000)
posterior = inference.build_posterior()

with open(rf"NPE_post_{max_num_trials}obs.pkl", "wb") as handle:
    pickle.dump(posterior, handle)

#load network
with open(rf'NPE_post_{max_num_trials}obs.pkl', 'rb') as f:
    posterior = pickle.load(f)

for xo in xos:
  for i in range(num_trials[0]):#this somehow limits the version of comparing different numbers of
    # we need to pad the x_os with NaNs to match the shape of the training data.
    xoi = torch.ones(1, max_num_trials, x_dim) * float("nan")
    xoi[0, 0:len(xo), :] = data[rf"{i}"][0:3]

num_samples = 1000
xos = [torch.zeros(nt, 3) for nt in num_trials]

npe_samples = []
# for xo in xos: ###original approach
#     # we need to pad the x_os with NaNs to match the shape of the training data.
#     xoi = torch.ones(1, max_num_trials, x_dim) * float("nan")
#     a = data[rf"{xo}"]
#     xoi[0, : len(xo), :] = torch.tensor(a)
#     npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))


for xo in xos:
  for i in range(num_trials[0]):#this somehow limits the version of comparing different numbers of
    # we need to pad the x_os with NaNs to match the shape of the training data.
    xoi = torch.ones(1, max_num_trials, x_dim) * float("nan")
    xoi[0, i, :] = data[rf"{i}"][0:3]
  npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))


# Plot them in one pairplot as contours (obtained via KDE on the samples).
fig, ax = pairplot(
    npe_samples,
    #points=torch.tensor(params),
    diag="kde",
    #upper="contour",
    kde_offdiag=dict(bins=50),
    kde_diag=dict(bins=100),
    contour_offdiag=dict(levels=[0.95]),
    points_colors=["k"],
    points_offdiag=dict(marker="*", markersize=10),
)
plt.sca(ax[1, 1])
plt.legend(
    [f"{nt} trials" if nt > 1 else f"{nt} trial" for nt in num_trials]
    + [r"$\theta_o$"],
    frameon=False,
    fontsize=12,
);
plt.savefig(rf"NPE_posterior_full_{max_num_trials}obs.png")

#posterior
posterior_samples = npe_samples[0]

fig, ax = analysis.pairplot(
    samples=posterior_samples,
    #limits=torch.tensor([[0.0, 2.0]] * D),
    #offdiag=["kde"],
    diag=["kde"],
    figsize=(8, 8),
    labels=[rf"$\theta_{d+1}$" for d in range(D)],
)
plt.savefig(rf"NPE_posterior_{max_num_trials}obs.png")

"""#### analysis"""

# i mendled quite a bit with the parameters in this cell, might not be suited well for changing conditions - if so, refer back to sbi tutorials

num_sbc_runs = 100  # choose a number of sbc runs, should be ~100s or ideally 1000
# generate ground truth parameters and corresponding simulated observations for SBC.
thetas = prior.sample((num_sbc_runs,))

#this version works altough I´m not sure yet if it does what it´s supposed to do
xs = torch.ones(num_sbc_runs, max_num_trials, x_dim) * float(
    "nan"
)
xi = simulator(theta[i].repeat(max_num_trials, 1))
for j in range(num_sbc_runs):
  xs[j, : j + 1, :] = xi[: j + 1, :]

#try to obtain more reasonable observations
xs = torch.ones(num_sbc_runs, max_num_trials, x_dim) * float(
    "nan"
)

for i in range(num_sbc_runs):
  #xi = simulator(theta[i].repeat(max_num_trials, 1))
  xi = simulator(theta[i].repeat(1,1))
  #for j in range(num_sbc_runs):
    #xs[j, : j + 1, :] = xi[: j + 1, :]
  xs[i, 0, :] = xi[0,:]

# run SBC: for each inference we draw 1000 posterior samples. #reduced to 100 instead of 1000 for time constraints
num_posterior_samples = 300
ranks, dap_samples = analysis.run_sbc(
    thetas, xs, posterior, num_posterior_samples=num_posterior_samples
)

check_stats = analysis.check_sbc(
    ranks, thetas, dap_samples, num_posterior_samples=num_posterior_samples
)

print(
    f"kolmogorov-smirnov p-values \ncheck_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}"
)

print(
    f"c2st accuracies \ncheck_stats['c2st_ranks'] = {check_stats['c2st_ranks'].numpy()}"
)

print(f"- c2st accuracies check_stats['c2st_dap'] = {check_stats['c2st_dap'].numpy()}")

f, ax = analysis.sbc_rank_plot(
    ranks=ranks,
    num_posterior_samples=num_posterior_samples,
    plot_type="hist",
    num_bins=10,  # by passing None we use a heuristic for the number of bins.
)
plt.savefig(rf"NPE_sbc_{max_num_trials}obs.png")

f, ax = analysis.sbc_rank_plot(ranks, 1_000, plot_type="cdf", num_bins = 10)
plt.savefig(rf"NPE_ECDF_{max_num_trials}obs.png")

#compute plot recovery, fixed on current prior
M = 100
z = np.zeros((D,M))
contr = np.zeros((D,M))

for k in range(M):

  #obtain new observation and draw samples
  new_npe_samples = []
  for xo in xos:
    # we need to pad the x_os with NaNs to match the shape of the training data.
    xoi = torch.ones(1, max_num_trials, x_dim) * float("nan")
    xoi[0, : len(xo), :] = xo
    new_npe_samples.append(posterior.sample(sample_shape=(num_samples,), x=xoi))
  poster_samples = new_npe_samples
  posterior_samples_ar = np.asarray(poster_samples)

  #compute empirical means/variances/std
  mean = np.zeros(D)
  var = np.zeros(D)
  std = np.zeros(D)
  var_prior = (0-50)**2/12

  for i in range(D):
    mean[i] = np.mean(posterior_samples_ar[:,i])
    var[i] = np.var(posterior_samples_ar[:,i])
    std[i] = np.std(posterior_samples_ar[:,i])

    #get scores
    z[i,k] = (mean[i]-params[i])/std[i]
    contr[i,k] = 1-var[i]/var_prior

for i in range(D):
  plt.scatter(contr[i,:],z[i,:])
  plt.title("Plot recovery " + rf"$\theta_{i+1}$")
  plt.xlabel("posterior contraction")
  plt.ylabel("posterior z-score")
  plt.xlim([0,1])
  plt.ylim([-3,3])
  plt.savefig(rf"NPE_plot_recovery_theta{1+i}_{max_num_trials}obs.png")
  plt.show()

